{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à kafka1:9092\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "def check_port(host, port):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    sock.settimeout(5)  # Timeout de 5 secondes\n",
    "    result = sock.connect_ex((host, port))\n",
    "    if result == 0:\n",
    "        print(f\"Connexion réussie à {host}:{port}\")\n",
    "    else:\n",
    "        print(f\"Impossible de se connecter à {host}:{port}\")\n",
    "    sock.close()\n",
    "\n",
    "check_port(\"kafka1\", 9092)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7f76866d-52c6-4eea-81d9-e55dab920b1c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 415ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.3 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   14  |   0   |   0   |   0   ||   14  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7f76866d-52c6-4eea-81d9-e55dab920b1c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 14 already retrieved (0kB/5ms)\n",
      "25/02/17 22:15:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/conda/lib/python3.12/site-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35306)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('SparkApp') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    " \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "\n",
    "# Créer un SQLContext pour les opérations SQL\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, count, window, avg, to_timestamp, min, max, last, broadcast, current_timestamp\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, DoubleType, BooleanType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schémas et Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_broker = \"kafka1:9092\"\n",
    "arrets_topic = \"arrets\"\n",
    "velos_topic = \"velos\"\n",
    "relais_topic = \"relais\"\n",
    "\n",
    "# Définition du schéma pour le topic \"arrets\"\n",
    "schema_arrets = StructType([\n",
    "    StructField(\"codeLieu\", StringType()),\n",
    "    StructField(\"libelle\", StringType()),\n",
    "    StructField(\"distance\", FloatType()),\n",
    "    StructField(\"ligne\", StringType())\n",
    "])\n",
    "\n",
    "# Définition du schéma pour le champ \"position\"\n",
    "position_schema = StructType([\n",
    "    StructField(\"lon\", DoubleType()),\n",
    "    StructField(\"lat\", DoubleType())\n",
    "])\n",
    "\n",
    "# Définition du schéma pour le topic \"velos\"\n",
    "schema_velos = StructType([\n",
    "    StructField(\"number\", StringType()),\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"address\", StringType()),\n",
    "    StructField(\"position\", position_schema),\n",
    "    StructField(\"banking\", StringType()),\n",
    "    StructField(\"bonus\", StringType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"contract_name\", StringType()),\n",
    "    StructField(\"bike_stands\", IntegerType()),\n",
    "    StructField(\"available_bike_stands\", IntegerType()),\n",
    "    StructField(\"available_bikes\", IntegerType()),\n",
    "    StructField(\"last_update\", StringType())\n",
    "])\n",
    "\n",
    "# Définition du schéma pour le topic \"relais\"\n",
    "schema_relais = StructType([\n",
    "    StructField(\"grp_identifiant\", StringType()),\n",
    "    StructField(\"grp_nom\", StringType()),\n",
    "    StructField(\"grp_statut\", IntegerType()),\n",
    "    StructField(\"grp_disponible\", IntegerType()),\n",
    "    StructField(\"grp_exploitation\", IntegerType()),\n",
    "    StructField(\"grp_complet\", IntegerType()),\n",
    "    StructField(\"grp_horodatage\", StringType()),\n",
    "    StructField(\"idobj\", StringType()),\n",
    "    StructField(\"location\", StructType([\n",
    "        StructField(\"lon\", DoubleType()),\n",
    "        StructField(\"lat\", DoubleType())\n",
    "    ])),\n",
    "    StructField(\"disponibilite\", DoubleType())\n",
    "])\n",
    "\n",
    "# Définition du schéma pour le topic \"relais\"\n",
    "schema_parking_public = StructType([\n",
    "    StructField(\"grp_identifiant\", StringType(), True),\n",
    "    StructField(\"grp_nom\", StringType(), True),\n",
    "    StructField(\"grp_statut\", StringType(), True),\n",
    "    StructField(\"grp_disponible\", IntegerType(), True),\n",
    "    StructField(\"grp_exploitation\", IntegerType(), True),\n",
    "    StructField(\"grp_complet\", IntegerType(), True),\n",
    "    StructField(\"grp_horodatage\", StringType(), True),\n",
    "    StructField(\"idobj\", StringType(), True),\n",
    "    StructField(\"location\", StructType([\n",
    "         StructField(\"lon\", DoubleType(), True),\n",
    "         StructField(\"lat\", DoubleType(), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requête Batch sans fenêtre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptage des arrêts par ligne à partir du topic « arrets »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:16:02 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|               ligne|nb_arrets|\n",
      "+--------------------+---------+\n",
      "| [{\"numLigne\":\"79\"}]|        6|\n",
      "|[{\"numLigne\":\"119...|        7|\n",
      "|[{\"numLigne\":\"105...|        4|\n",
      "|[{\"numLigne\":\"127...|        2|\n",
      "|[{\"numLigne\":\"10\"...|        1|\n",
      "| [{\"numLigne\":\"91\"}]|       14|\n",
      "|[{\"numLigne\":\"75\"...|        6|\n",
      "|[{\"numLigne\":\"50\"...|        4|\n",
      "|[{\"numLigne\":\"101...|        2|\n",
      "| [{\"numLigne\":\"30\"}]|       12|\n",
      "| [{\"numLigne\":\"80\"}]|       10|\n",
      "|[{\"numLigne\":\"2B\"...|        1|\n",
      "|[{\"numLigne\":\"112...|        1|\n",
      "|[{\"numLigne\":\"59\"...|        3|\n",
      "|[{\"numLigne\":\"172...|        2|\n",
      "|[{\"numLigne\":\"30\"...|        7|\n",
      "|[{\"numLigne\":\"10\"...|        1|\n",
      "|[{\"numLigne\":\"115\"}]|        1|\n",
      "|[{\"numLigne\":\"179...|        2|\n",
      "|[{\"numLigne\":\"107...|        3|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:16:07 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 3:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+-----------------+\n",
      "|summary|               ligne|        nb_arrets|\n",
      "+-------+--------------------+-----------------+\n",
      "|  count|                 434|              434|\n",
      "|   mean|                NULL|2.686635944700461|\n",
      "| stddev|                NULL|3.224968441931372|\n",
      "|    min|                  []|                1|\n",
      "|    max|[{\"numLigne\":\"NBI\"}]|               28|\n",
      "+-------+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lecture brute des messages Kafka\n",
    "df_kafka = sql_context.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", arrets_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du champ \"value\" (encodé en JSON) en colonnes structurées\n",
    "df_arrets = df_kafka.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema_arrets).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Agrégation : compter le nombre d'arrêts par \"ligne\"\n",
    "result = df_arrets.groupBy(\"ligne\").agg(count(\"*\").alias(\"nb_arrets\"))\n",
    "\n",
    "# Affichage du résultat\n",
    "result.show()\n",
    "result.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des stations de vélo par statut historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:16:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/17 22:16:15 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:16:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 22:16:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 22:16:15 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 22:16:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 22:16:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+--------------------+------------------+-----------+\n",
      "|status|total_stations|moyenne_velos_dispos|  ecart_type_velos|pourcentage|\n",
      "+------+--------------+--------------------+------------------+-----------+\n",
      "|  OPEN|          4606|   8.278766825879288|6.2154943408644705|       0.98|\n",
      "|CLOSED|            94|                 0.0|               0.0|       0.02|\n",
      "+------+--------------+--------------------+------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:16:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/02/17 22:16:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BikeHistory\").getOrCreate()\n",
    "\n",
    "# --- Helper function: read_kafka_stream ---\n",
    "def read_kafka_stream(topic, schema, is_streaming=True):\n",
    "    if is_streaming:\n",
    "        df = spark.readStream.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "            .option(\"subscribe\", topic) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load()\n",
    "    else:\n",
    "        df = spark.read.format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "            .option(\"subscribe\", topic) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .load()\n",
    "    # Convert the binary \"value\" column to string and parse JSON using the provided schema.\n",
    "    df = df.selectExpr(\"CAST(value AS STRING) as json_string\")\n",
    "    df = df.select(F.from_json(F.col(\"json_string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "    return df\n",
    "\n",
    "# Read batch data from the \"velos\" topic (is_streaming=False)\n",
    "df_velos = read_kafka_stream(\"velos\", schema_velos, is_streaming=False).where(F.col(\"status\").isNotNull())\n",
    "\n",
    "# Perform aggregation by 'status'\n",
    "result_batch1 = df_velos.groupBy(\"status\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_stations\"),\n",
    "        F.avg(\"available_bikes\").alias(\"moyenne_velos_dispos\"),\n",
    "        F.stddev(\"available_bikes\").alias(\"ecart_type_velos\")\n",
    "    ) \\\n",
    "    .withColumn(\"pourcentage\", F.col(\"total_stations\") / F.sum(\"total_stations\").over(Window.partitionBy())) \\\n",
    "    .orderBy(F.desc(\"total_stations\"))\n",
    "\n",
    "result_batch1.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requête Batch avec fenêtre\n",
    "\n",
    "Vélos disponibles par contrat sur des fenêtres temporelles à partir du topic « velos ».\n",
    "Malheuresement on a qu'une donnée par station donc ca permet pas de faire de l'aggrégation comme la moyenne de vélo sur la période..... faut essayer de trouver des données plus pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:16:54 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 15:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------+----------+----------------------+\n",
      "|              window|number|contract_name|nb_updates|latest_available_bikes|\n",
      "+--------------------+------+-------------+----------+----------------------+\n",
      "|{2022-05-23 14:30...|  1011|       nantes|        48|                     0|\n",
      "|{2025-02-17 19:30...|    34|       nantes|        28|                     6|\n",
      "|{2025-02-17 19:00...|     5|       nantes|         6|                     2|\n",
      "|{2025-02-17 19:30...|    70|       nantes|        36|                    50|\n",
      "|{2025-02-17 19:00...|    59|       nantes|         6|                     3|\n",
      "|{2025-02-17 19:30...|    90|       nantes|        34|                     4|\n",
      "|{2025-02-17 19:00...|    72|       nantes|         6|                     7|\n",
      "|{2025-02-17 19:30...|    83|       nantes|        18|                     6|\n",
      "|{2025-02-17 19:30...|   102|       nantes|        30|                    15|\n",
      "|{2025-02-17 19:30...|   103|       nantes|        28|                    13|\n",
      "|{2025-02-17 19:30...|    35|       nantes|        25|                    13|\n",
      "|{2025-02-17 19:30...|   114|       nantes|        26|                    13|\n",
      "|{2025-02-17 19:30...|   100|       nantes|        16|                     7|\n",
      "|{2025-02-17 19:30...|   121|       nantes|        10|                     9|\n",
      "|{2025-02-17 19:30...|    14|       nantes|        10|                    11|\n",
      "|{2025-02-17 20:00...|     9|       nantes|        15|                     8|\n",
      "|{2025-02-17 20:00...|    10|       nantes|         9|                     9|\n",
      "|{2025-02-17 20:00...|   115|       nantes|        13|                    12|\n",
      "|{2025-02-17 20:00...|    24|       nantes|        15|                     5|\n",
      "|{2025-02-17 20:00...|    55|       nantes|        13|                     7|\n",
      "+--------------------+------+-------------+----------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lecture brute des messages Kafka\n",
    "df_kafka = sql_context.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", velos_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du champ \"value\" en JSON\n",
    "df_velos = df_kafka.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema_velos).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Conversion de \"last_update\" en timestamp (format ISO 8601)\n",
    "df_velos = df_velos.withColumn(\"last_update_ts\", \n",
    "                               to_timestamp(col(\"last_update\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\"))\n",
    "\n",
    "# Agrégation par fenêtre (ici, on allonge la fenêtre à 30 minutes pour accumuler plusieurs mises à jour) et par station\n",
    "result_window = df_velos.groupBy(\n",
    "    window(col(\"last_update_ts\"), \"30 minutes\"), \n",
    "    col(\"number\"),\n",
    "    col(\"contract_name\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"nb_updates\"),\n",
    "    last(\"available_bikes\", ignorenulls=True).alias(\"latest_available_bikes\")\n",
    ")\n",
    "\n",
    "result_window.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requête Batch avec visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphique des totaux de stations par statut avec Pandas et Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert the Spark DataFrame to Pandas for visualization\n",
    "pdf = result_batch1.toPandas()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(data=pdf, x=\"status\", y=\"total_stations\", palette=\"viridis\")\n",
    "plt.title(\"Total des stations de vélo par statut\")\n",
    "plt.xlabel(\"Statut\")\n",
    "plt.ylabel(\"Nombre total de stations\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requêtes Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuperer la moyenne des disponiblités des parkings relais sur une fenêtre glissante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture en streaming depuis Kafka\n",
    "df_kafka = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", relais_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du champ \"value\" (JSON) en colonnes structurées\n",
    "df_relais = df_kafka.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema_relais).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Conversion du champ \"grp_horodatage\" en timestamp\n",
    "# Le format ISO 8601 est utilisé : \"yyyy-MM-dd'T'HH:mm:ssXXX\"\n",
    "df_relais = df_relais.withColumn(\"horodatage_ts\", \n",
    "                                  to_timestamp(col(\"grp_horodatage\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\"))\n",
    "\n",
    "# Définition d'un watermark de 5 minutes pour gérer les retards éventuels\n",
    "df_relais = df_relais.withWatermark(\"horodatage_ts\", \"5 minutes\")\n",
    "\n",
    "# Agrégation : calcul de la moyenne des disponibilités sur une fenêtre glissante\n",
    "# Ici, la fenêtre est de 5 minutes et on agrège par \"grp_identifiant\"\n",
    "result_stream = df_relais.groupBy(\n",
    "    window(col(\"horodatage_ts\"), \"5 minutes\"),\n",
    "    col(\"grp_identifiant\"),\n",
    ").agg(\n",
    "    avg(\"disponibilite\").alias(\"avg_disponibilite\")\n",
    ")\n",
    "\n",
    "# Écriture du résultat en streaming vers la console (mode \"update\")\n",
    "query = result_stream.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuperer la moyenne des vélos disponibles par parking sur une fenêtre glissante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture en streaming depuis Kafka\n",
    "df_kafka_velos = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", velos_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Conversion du champ \"value\" (JSON) en colonnes structurées\n",
    "df_velos = df_kafka_velos.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema_velos).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Conversion du champ \"last_update\" en timestamp (format ISO 8601)\n",
    "df_velos = df_velos.withColumn(\"last_update_ts\", \n",
    "                               to_timestamp(col(\"last_update\"), \"yyyy-MM-dd'T'HH:mm:ssXXX\"))\n",
    "\n",
    "# Application d'un watermark de 5 minutes pour gérer les retards\n",
    "df_velos = df_velos.withWatermark(\"last_update_ts\", \"5 minutes\")\n",
    "\n",
    "# Agrégation sur une fenêtre de 5 minutes : calcul du total des places disponibles\n",
    "result_velos = df_velos.groupBy(\n",
    "    window(col(\"last_update_ts\"), \"5 minutes\"),\n",
    "    col(\"name\")\n",
    ").agg(\n",
    "    avg(\"available_bikes\").alias(\"moyenne_velos_disponibles\")\n",
    ")\n",
    "\n",
    "# Écriture du résultat en streaming vers la console (mode \"update\")\n",
    "query_velos = result_velos.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query_velos.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_velos.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Batch Query – Aggregated Multimodal Connectivity Hubs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:29:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:29:59 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:30:00 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No matching bike stations found near full parkings\n"
     ]
    }
   ],
   "source": [
    "# Batch Request: Bike Availability Near Full Public Parkings (Optimized)\n",
    "# ======================================================================\n",
    "\n",
    "# 1. Read and Validate Data\n",
    "# -------------------------\n",
    "# Read parking data with correct topic name\n",
    "df_parking = read_kafka_stream(\n",
    "    \"parking_public\",  # Match actual topic name\n",
    "    schema_parking_public, \n",
    "    is_streaming=False\n",
    ").filter(\n",
    "    (F.col(\"location.lon\").isNotNull()) &\n",
    "    (F.col(\"location.lat\").isNotNull()) &\n",
    "    (F.col(\"grp_complet\") == 1)  # Directly filter full parkings\n",
    ").alias(\"parking\")\n",
    "\n",
    "# Read bike data with quality checks\n",
    "df_velos = read_kafka_stream(\n",
    "    velos_topic, \n",
    "    schema_velos, \n",
    "    is_streaming=False\n",
    ").filter(\n",
    "    (F.col(\"position.lon\").isNotNull()) &\n",
    "    (F.col(\"position.lat\").isNotNull()) &\n",
    "    (F.col(\"available_bikes\") > 0)\n",
    ").alias(\"bike\")\n",
    "\n",
    "# 2. Spatial Pre-filtering\n",
    "# ------------------------\n",
    "# Approximate bounding box filter (0.5° ~55km at equator)\n",
    "parking = df_parking.withColumn(\"parking_geo\", F.struct(\"location.lon\", \"location.lat\"))\n",
    "bikes = df_velos.withColumn(\"bike_geo\", F.struct(\"position.lon\", \"position.lat\"))\n",
    "\n",
    "# 3. Optimized Spatial Join\n",
    "# -------------------------\n",
    "join_condition = [\n",
    "    F.abs(parking[\"location.lon\"] - bikes[\"position.lon\"]) <= 0.5,\n",
    "    F.abs(parking[\"location.lat\"] - bikes[\"position.lat\"]) <= 0.5\n",
    "]\n",
    "\n",
    "joined_df = parking.join(\n",
    "    broadcast(bikes),\n",
    "    join_condition,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "# 4. Precise Distance Calculation\n",
    "# -------------------------------\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"distance_km\",\n",
    "    haversine_udf(\n",
    "        F.col(\"location.lon\"), \n",
    "        F.col(\"location.lat\"),\n",
    "        F.col(\"position.lon\"),\n",
    "        F.col(\"position.lat\")\n",
    "    )\n",
    ").filter(F.col(\"distance_km\") <= 0.5)\n",
    "\n",
    "# 5. Aggregation with Monitoring\n",
    "# ------------------------------\n",
    "if joined_df.rdd.isEmpty():\n",
    "    print(\"Warning: No matching bike stations found near full parkings\")\n",
    "else:\n",
    "    parking_bike_availability = (joined_df\n",
    "        .groupBy(\"grp_identifiant\", \"grp_nom\")\n",
    "        .agg(\n",
    "            F.sum(\"available_bikes\").alias(\"total_bikes\"),\n",
    "            F.count_distinct(\"number\").alias(\"distinct_stations\")\n",
    "        )\n",
    "        .withColumn(\"status\", \n",
    "            F.when(F.col(\"total_bikes\") >= 20, \"Sufficient\")\n",
    "            .otherwise(\"Insufficient\"))\n",
    "        .orderBy(F.desc(\"total_bikes\"))\n",
    "    )\n",
    "\n",
    "    # Show execution plan\n",
    "    parking_bike_availability.explain()\n",
    "    \n",
    "    # Show results\n",
    "    parking_bike_availability.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:51:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:51:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:51:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:51:10 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:51:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:51:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 60:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+------------------------+-----------+---------------+------------------+------------------+------------------------+\n",
      "|grp_identifiant|grp_nom               |available_parking_spaces|total_bikes|nearby_stations|closest_station_km|closest_station_id|bike_availability_status|\n",
      "+---------------+----------------------+------------------------+-----------+---------------+------------------+------------------+------------------------+\n",
      "|003            |Tour Bretagne         |440                     |243175     |19             |0.1               |6                 |Good                    |\n",
      "|005            |Aristide Briand       |200                     |232850     |14             |0.01              |15                |Good                    |\n",
      "|008            |Talensac              |0                       |225925     |15             |0.06              |19                |Good                    |\n",
      "|035            |Baco-LU 1             |63                      |198875     |13             |0.02              |52                |Good                    |\n",
      "|002            |Decré-Bouffay         |373                     |193700     |16             |0.03              |4                 |Good                    |\n",
      "|021            |Gare Nord             |435                     |193150     |11             |0.13              |60                |Good                    |\n",
      "|004            |Graslin               |325                     |190650     |13             |0.1               |33                |Good                    |\n",
      "|012            |Descartes             |149                     |185825     |11             |0.1               |15                |Good                    |\n",
      "|009            |Cité des Congrès      |403                     |178025     |9              |0.08              |54                |Good                    |\n",
      "|016            |Gare Sud 2 : Limité 1h|39                      |162600     |7              |0.03              |59                |Good                    |\n",
      "|027            |Gare Sud 3            |206                     |158575     |7              |0.15              |59                |Good                    |\n",
      "|029            |Bellamy               |75                      |157450     |10             |0.11              |71                |Good                    |\n",
      "|007            |Commerce              |297                     |152800     |14             |0.06              |29                |Good                    |\n",
      "|034            |Gare Sud 4            |0                       |134550     |6              |0.2               |79                |Good                    |\n",
      "|001            |Feydeau               |366                     |128000     |15             |0.03              |20                |Good                    |\n",
      "|010            |Cathédrale            |158                     |110250     |11             |0.19              |32                |Good                    |\n",
      "|036            |Baco-LU 2             |75                      |105700     |14             |0.16              |20                |Good                    |\n",
      "|006            |Médiathèque           |225                     |82250      |10             |0.04              |34                |Good                    |\n",
      "|014            |Gloriette 1           |57                      |64150      |10             |0.11              |31                |Good                    |\n",
      "|015            |Hôtel Dieu            |56                      |58800      |9              |0.14              |38                |Good                    |\n",
      "+---------------+----------------------+------------------------+-----------+---------------+------------------+------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Batch Request: Deduplicated Bike Availability Analysis\n",
    "# ======================================================\n",
    "\n",
    "# 1. Window Function with Row Number\n",
    "window_spec = Window.partitionBy(\"grp_identifiant\").orderBy(\n",
    "    \"distance_km\",\n",
    "    F.desc(\"bike.available_bikes\")  # Tiebreaker: prefer stations with more bikes\n",
    ")\n",
    "\n",
    "closest_stations = (joined_df\n",
    "    .withColumn(\"closest_rank\", F.row_number().over(window_spec))\n",
    "    .filter(F.col(\"closest_rank\") == 1)\n",
    "    .select(\n",
    "        \"grp_identifiant\",\n",
    "        \"grp_nom\",\n",
    "        F.col(\"parking.grp_disponible\").alias(\"available_parking_spaces\"),\n",
    "        F.col(\"bike.number\").alias(\"closest_station_id\"),\n",
    "        F.round(\"distance_km\", 2).alias(\"closest_station_km\")\n",
    "    )\n",
    "    .distinct()  # Add distinct to remove potential duplicates\n",
    ")\n",
    "\n",
    "# 2. Aggregation with Deduplication\n",
    "nearby_counts = (joined_df\n",
    "    .groupBy(\"grp_identifiant\")\n",
    "    .agg(\n",
    "        F.count_distinct(\"bike.number\").alias(\"nearby_stations\"),\n",
    "        F.sum(\"bike.available_bikes\").alias(\"total_bikes\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Final Join with Deduplication Check\n",
    "final_result = (closest_stations\n",
    "    .join(nearby_counts, \"grp_identifiant\")\n",
    "    .withColumn(\"bike_availability_status\",\n",
    "                F.when(F.col(\"total_bikes\") >= 15, \"Good\")\n",
    "                 .when(F.col(\"total_bikes\") >= 5, \"Moderate\")\n",
    "                 .otherwise(\"Low\"))\n",
    "    .select(\n",
    "        \"grp_identifiant\",\n",
    "        \"grp_nom\",\n",
    "        \"available_parking_spaces\",\n",
    "        \"total_bikes\",\n",
    "        \"nearby_stations\",\n",
    "        \"closest_station_km\",\n",
    "        \"closest_station_id\",\n",
    "        \"bike_availability_status\"\n",
    "    )\n",
    "    .dropDuplicates([\"grp_identifiant\"])  # Ensure final deduplication\n",
    "    .orderBy(F.desc(\"total_bikes\"))\n",
    ")\n",
    "\n",
    "final_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:56:34 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:56:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parking Count: 675\n",
      "Velos Count: 6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 22:56:36 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 22:56:36 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relais Count: 7000\n",
      "Non-Full Parkings: 675\n"
     ]
    }
   ],
   "source": [
    "# Check record counts at each stage\n",
    "print(\"Parking Count:\", df_parking.count())  # Should be >0\n",
    "print(\"Velos Count:\", df_velos.count())      # Should be >0\n",
    "print(\"Relais Count:\", df_relais.count())    # Should be >0\n",
    "\n",
    "# Check non-full parkings\n",
    "non_full = df_parking.filter(F.col(\"grp_complet\") != 1)\n",
    "print(\"Non-Full Parkings:\", non_full.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/17 23:20:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 23:20:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/02/17 23:20:01 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 102:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+---------------+-------------------+------------------+\n",
      "|location_name|avg_bikes         |nearby_stations|lon                |lat               |\n",
      "+-------------+------------------+---------------+-------------------+------------------+\n",
      "|004          |10.725738396624472|13             |-1.562571814000023 |47.214310205000004|\n",
      "|015          |5.321266968325792 |9              |-1.5519831099999806|47.21105707999999 |\n",
      "|017          |4.243161094224924 |8              |-1.5572502320000012|47.20936196600002 |\n",
      "|047          |6.5886075949367084|4              |-1.5632873819999986|47.20409254200001 |\n",
      "|006          |7.580645161290323 |10             |-1.56233817399999  |47.210870821000015|\n",
      "|009          |12.994525547445255|9              |-1.5439712740000004|47.212901665000004|\n",
      "|001          |6.221142162818955 |15             |-1.552558781000016 |47.21407529499999 |\n",
      "|042          |5.213793103448276 |7              |-1.55291388400002  |47.20889024799999 |\n",
      "|035          |10.536423841059603|13             |-1.5489888249999808|47.214332415      |\n",
      "|014          |5.9953271028037385|10             |-1.5584387270000093|47.21117964899997 |\n",
      "|027          |14.126948775055679|7              |-1.539256308000006 |47.21491533400001 |\n",
      "|037          |6.509859154929577 |8              |-1.560284366000019 |47.21027368099999 |\n",
      "|044          |6.343396226415094 |5              |-1.5363160630000152|47.216981170999986|\n",
      "|010          |6.763803680981595 |11             |-1.5510561649999772|47.22081334400002 |\n",
      "|040          |5.28              |4              |-1.5443329510000012|47.20497045899998 |\n",
      "|007          |8.21505376344086  |14             |-1.557713217000014 |47.21312028       |\n",
      "|008          |9.876502732240438 |15             |-1.5583572969999864|47.220255974      |\n",
      "|002          |7.5886385896180215|16             |-1.5540046290000191|47.21666261299998 |\n",
      "|012          |12.793459552495698|11             |-1.5643645260000199|47.21718884799998 |\n",
      "|011          |1.9745762711864407|4              |-1.5627277899999967|47.20590061600001 |\n",
      "+-------------+------------------+---------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Batch Request: Average Bike Availability Near Available Parkings\n",
    "parking_bike_analysis = (\n",
    "    df_parking.alias(\"parking\")\n",
    "    .filter(F.col(\"parking.grp_complet\") != 1)\n",
    "    .join(\n",
    "        df_velos.alias(\"velo\"),\n",
    "        [  # Properly balanced parentheses\n",
    "            (F.abs(F.col(\"parking.location.lon\") - F.col(\"velo.position.lon\")) <= 0.01),\n",
    "            (F.abs(F.col(\"parking.location.lat\") - F.col(\"velo.position.lat\")) <= 0.01)\n",
    "        ]\n",
    "    )\n",
    "    .withColumn(\"distance_km\", haversine_udf(\n",
    "        F.col(\"parking.location.lon\"), \n",
    "        F.col(\"parking.location.lat\"),\n",
    "        F.col(\"velo.position.lon\"),\n",
    "        F.col(\"velo.position.lat\")\n",
    "    ))\n",
    "    .filter(F.col(\"distance_km\") <= 0.5)\n",
    "    .groupBy(\"parking.grp_identifiant\")\n",
    "    .agg(\n",
    "        F.avg(\"velo.available_bikes\").alias(\"avg_bikes\"),\n",
    "        F.count_distinct(\"velo.number\").alias(\"nearby_stations\"),\n",
    "        F.first(\"parking.location.lon\").alias(\"parking_lon\"),\n",
    "        F.first(\"parking.location.lat\").alias(\"parking_lat\")\n",
    "    )\n",
    "    .join(df_relais.alias(\"relais\"), \"grp_identifiant\", \"left\")\n",
    "    .select(\n",
    "        F.coalesce(\"relais.grp_nom\", \"grp_identifiant\").alias(\"location_name\"),\n",
    "        \"avg_bikes\",\n",
    "        \"nearby_stations\",\n",
    "        F.coalesce(\"relais.location.lon\", \"parking_lon\").alias(\"lon\"),\n",
    "        F.coalesce(\"relais.location.lat\", \"parking_lat\").alias(\"lat\")\n",
    "    )\n",
    ")\n",
    "\n",
    "parking_bike_analysis.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
